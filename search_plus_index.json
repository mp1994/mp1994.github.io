{"/2boot/": {
    "title": "2boot",
    "keywords": "2boot",
    "url": "/2boot/",
    "body": ""
  },"/linux/": {
    "title": "linux",
    "keywords": "linux",
    "url": "/linux/",
    "body": ""
  },"/meta/": {
    "title": "meta",
    "keywords": "meta",
    "url": "/meta/",
    "body": ""
  },"/pages/whoami/": {
    "title": "whoami",
    "keywords": "meta",
    "url": "/pages/whoami/",
    "body": "So, this is it. After a very long time offline, I finally went back to blogging. I liked the concept of this being a GitHub repository. I’m probably doing this wrong, but I also liked Jekyll and wanted to experiment it. I am not sure I can easily go back to HTML/PHP/MySQL as if all these years hadn’t passed. mp1994.get_whoami() I am Mattia, $year - 1994 years old, from Milan, Italy. I am biomedical engineer, and I am currently a PhD Student in Bioengineering at the Politecnico di Milano. But I don’t think I will talk about this here. I am a tech enthusiast, petrolhead, meme enjoyer. I have a thing for The Office. You can find me on TV Time (mp.1994) to check out what other shows I like. I started this blog mainly to complement some projects that I recently started to publish on my Github. Speaking of which, for now I am ending this page (I feel I’ve already talked too much about me) with my GitHub chart 1.  "
  },"/linux/2023-02-07-deterministic-loops.html": {
    "title": "Deterministic and reliable execution of loop functions in C/C++",
    "keywords": "linux, embedded, loop function, timer, deadline engineering linux",
    "url": "/linux/2023-02-07-deterministic-loops.html",
    "body": "This post illustrates how to execute in a reliable and deterministic way a function at a (fixed) frequency, i.e. after a user-defined time interval that we’ll call cycle time. In the following, we will implement a basic loop function with timer re-arming and self-benchmarking features, and we will wrap it to hide this functionalities to the end user, allowing them to only change the user code that will be executed in the loop. For variable cycle durations, or to implement an adaptive frequency (e.g., switch between idle state and active state), we could also expose to the user timer re-arming. As always, the implementation and the code shown below are tested on Linux only and are not guaranteed to work under any other OS. SIGALRM Under Linux, POSIX provides the SIGALRM signal that fires when a timer expires. We can exploit the alarm(int seconds) function to set the SIGALRM after a specified time interval in seconds. This can execute any function we previously assigned to this signal. // Loop function void loop_function(int s) { } // Link our callback to the SIGALRM signal signal(SIGALRM, loop_function); // Set the timer to expire in 2 seconds alarm(2); The main limitation of this approach is that alarm() takes an integer (unsigned int) and thus we cannot achieve sub-second precision. One could use setitimer() instead, but we are not going to do that. Instead, we are going to take advantage of the Boost library. Boost ASIO Asynchronous input/output (ASIO) allows concurrent, non-blocking I/O operations to run without affecting the main program. In my (limited) experience, Boost.Asio is the way to go for C++ applications that require asynchronous I/O. The documentation also contains useful tutorials and examples. Please mind that I have been posting links to Boost version 1.65.1 since it is the version installed by apt on Ubuntu 18.04 LTS. deadline_timer The basic_deadline_timer class template provides the ability to perform a blocking or asynchronous wait for a timer to expire. The wait() method can be used to wait for the timer to expire while blocking the execution of the current thread, while we can use async_wait() to avoid blocking thread’s execution while waiting for the deadline timer to expire. To create a deadline timer, we need few lines of code: // IO service that handles the deadline_timer boost::asio::io_service timer_io; // Create the timer object boost::asio::deadline_timer t(timer_io); // Set the expire time relative to the current time t.expires_from_now(boost::posix_time::seconds(5)); // Non-blocking wait t.async_wait(&amp;loop_function); // Start the timer // The IO service run() is blocking, so the main thread // will be blocked by this function call. timer_io.run(); You can find the full code of the example above and run it here. There are some things that are worth mentioning. Every deadline_timer needs an io_service to run. One or more timers can be assigned to the same IO service when they are created. Although the call to the method async_wait() is non-blocking, if we are going to run the run() method of the IO service, the main thread will be blocked. This means that we need to wrap the call to the IO service in another thread to fully exploit the deadline timer to generate a non-blocking loop. The example code above sets a deadline timer to fire after 5 seconds and calls the handler function loop_function after waiting. Hence, the timer only fires once, timer_io.run() returns and the program exits. To effectively run a loop, we need to re-arm the timer, and we can do this directly inside the loop_function. void loop_function(const boost::system::error_code&amp; /*e*/) { // Re-arm the timer t.expires_from_now(boost::posix_time::seconds(5)); t.async_wait(&amp;loop_function); /** User code **/ std::cout &lt;&lt; \"deadline_timer expired\\n\"; /** User code **/ } In this way, the callback first resets the timer, and then it actually executes the user code. In this example, the timer object t must be a global variable, or we must use a global (shared) pointer. We may want to hide the code that handles the timer itself to the user and only provide a function that gets called at every cycle. If this is the case, we can wrap a user-defined loop function into the actual callback that gets called by the deadline timer. static void w_loop_function(const boost::system::error_code&amp; /*e*/) { // Re-arm the timer t.expires_from_now(boost::posix_time::seconds(5)); t.async_wait(&amp;w_loop_function); // Call the user-defined loop function loop_function(); // Update the loop counter loop_counter++; } void loop_function(void) { /** User code **/ std::cout &lt;&lt; \"deadline_timer expired\\n\"; /** User code **/ } In this way, the w_loop_function can be hidden to the final user of our API, exposing only a template loop_function that can be defined as required by the application. In the case we may want to achieve a variable loop frequency, we could have a global variable to be used as the argument of the call t.expires_from_now(), or – even better – we could put everything into a class. This example shows the implementation of a custom deadline timer class with a wrapped loop function to handle timer re-arming and benchmarking the effective cycle time duration. Moreover, it sets real-time priority (sched_priority = 99) with FIFO scheduling to the dedicated thread that runs the IO service. Why the f is this slower with the PREEMPT_RT kernel? That’s still an open question for me: when I initially started developing and testing the code based on the boost::asio::deadline_timer, I was using the mainline Linux kernel. I got very promising results, as the ones reported above, so I was thrilled when I first tested it on a custom kernel with the PREEMPT_RT patch. I was disappointed soon after. The loop is supposed to run at 1000 Hz, yet the average cycle time is 1.33 ms, with all the cycles failing to respect the 1 ms interval. Why? Well, boost::asio::deadline_timer is not actually using a hardware timer. The ASIO service created to run the deadline_timer asynchronously waits for the expire time, while letting other tasks and functions run in the meantime. At the expire time, the user-defined callback is executed. It is slower because the boost::asio::deadline_timer is a general-purpose timer implementation that is designed to work with any operating system and is not specifically optimized for real-time systems. Hence, it does not and cannot use real-time scheduling. Wrap-up The boost::asio::deadline_timer is a great and versatile approach to have soft real-time loop functions in a general-purpose OS. We can achieve a reliable 1000 Hz loop on the mainline Linux kernel. Of course, this does not guarantee limited and predictable latency, so for hard real-time applications we need the PREEMPT_RT patch and ad-hoc real-time software. A note on Boost and portability I am a fan of the Boost library, which I consider one of the best open-source projects. Yet, preparing the example code for this post I was a bit disappointed. I was familiar with wandbox, an online C++ compiler that lets you play around with the code, build it and run it online. It is great for sharing code with people and for quick demos, as I intended to do here. Wandbox lets you choose the compiler, and I chose gcc 7.5.0, the same I have installed on my machine. I could not choose the version of Boost, which is 1.75.0. Linking boost_thread and boost_system works fine and the first example code run as smoothly as on my laptop (the output is buffered so it won’t update during the execution of the program…). Things went differently when I copy-pasted the code of the second example. Turns out boost::posix_time::milliseconds() cannot take a non-integer argument, and so my code could not compile on Wandbox. So basically, I need to convert to integer the cycle time dt before converting it to milliseconds, effectively limiting the frequency of the loops to 1000 Hz. Fortunately, I could change my code and use instead boost::posix_time::microseconds(), that limits the maximum loop frequency to 1 MHz. Although modern CPUs clock faster than 1 GHz, on my laptop I could not achieve a rate higher than 10 kHz with reliable cycle times. I further investigated what I believed was a portability issue. Newer versions of Boost, like the 1.75.0, provide mostly header-only libraries that do not require compilation, which is great for a service like Wandbox as it reduces the computation cost of the build process. It turns out that the difference between the two versions of Boost was indeed a bugfix that also fixed my code: boost::posix_time::milliseconds() (as well as microseconds() and seconds()) could take a non-integer argument, but then it could not convert it to the proper time, thus generating the wrong loop rate. Published on February 7, 2023 --- Last modified February 10, 2023"
  },"/linux/2022-09-05-preempt-patch.html": {
    "title": "Real-time PREEMPT patch for the Linux Kernel",
    "keywords": "linux, rt, preempt linux",
    "url": "/linux/2022-09-05-preempt-patch.html",
    "body": "This article provides a minimal guide to installing the PREEMPT RT patch for the Linux kernel on a general-purpose computer (Dell XPS 7390). This should work on any modern PC/laptop. Preamble: Real-Time applications Computers may be used to interface or control real-time systems, i.e. systems that must react to an external event like an interrupt within a defined time frame [1]. Hence, such systems must always respond with a limited and deterministic delay to external events. Low-priority tasks must be preempted (i.e., interrupted) to meet such constrained deadlines; such tasks are then completed by the CPU once the higher-priority, real-time task is complete. The main aim of the PREEMPT_RT patch is to minimize the amount of kernel code that is non-preemptible [2]. A typical real-time application is running a control loop at a specific frequency. Kernel-level preemption allows the CPU to execute the loop at the specified rate (below a certain limit). Moreover, the PREEMPT_RT patch also enables high-resolution timers allowing precise timed scheduling. 1- Install required tools sudo apt install build-essential bc curl \\ wget ca-certificates gnupg2 libssl-dev \\ lsb-release libelf-dev bison flex dwarves \\ zstd libncurses-dev dwarves 2- Prepare the build environment We are going to create a folder in the $HOME folder, where we will apply the patch and compile the kernel. cd mkdir kernel &amp;&amp; cd kernel 3- Download the kernel and the corresponding patch This guide was written after patching kernel v5.4.19 with the PREEMPT RT patch version rt-11. We can download the kernel and the patch using wget, or browsing the public repository for the kernel and the patch, respectively. wget https://mirrors.edge.kernel.org/pub/linux/kernel/v5.x/linux-5.4.19.tar.xz wget https://mirrors.edge.kernel.org/pub/linux/kernel/projects/rt/5.4/older/patch-5.4.19-rt11.patch.xz 4- Extract the archives and apply the patch xz -d *.xz tar xf linux-*.tar cd linux-*/ patch -p1 &lt; ../patch-*.patch The commands above use the regex wildcard * and thus will work independently of the version we are patching. 5- Configure the kernel First, we are going to copy our current configuration from the boot partition cp -v /boot/config-$(uname -r) .config In the command above, $(uname -r) expands to the current kernel version (e.g., 5.4.0-124-generic in my case) and copies it to the local file .config. Now, we need to configure the kernel. Specifically, we need to set the preemption model to enable the full real-time capability. make olddefconfig make menuconfig The latest command will open a TUI (terminal user interface): navigate with the arrow keys and select: General Setup &gt; Preemption Model &gt; Fully Preemptible Kernel (Real-Time) Cryptographic API &gt; Certificates for signature checking (at the very bottom of the list) &gt; Provide system-wide ring of trusted keys &gt; Additional X.509 keys for default system keyring &gt; \"\" (i.e., remove \"debian/canonical-certs.pem\" from the text input field) Save to .config and exit. 6- Build the kernel as a Debian package and install with dpkg make -j$(nproc) deb-pkg sudo dpkg -i ../linux-headers-*.deb ../linux-image-*.deb 7- Reboot and verify The install process should have created the initial ramdisk file and the kernel file, and added the new boot entry to GRUB. We can verify with ls -l /boot | grep rt. $ ls -l /boot/ | grep rt config-5.4.19-rt11preempt initrd.img-5.4.19-rt11preempt System.map-5.4.19-rt11preempt vmlinuz-5.4.19-rt11preempt We can now reboot and select the RT kernel from GRUB. To verify all is set correctly, we can check the output of uname -a and verify that cat /sys/kernel/realtime returns 1. Note 1: UEFI and Secure Boot Modern systems equipped with UEFI firmware (i.e., the new BIOS) often have secure boot enabled by default. This means that the system will boot only if the kernel is signed. This may be tricky to achieve when building our own kernel, as in this case. I therefore suggest to disable secure boot (especially in case of boot issues after trying out this guide). Note 2: RT permissions It may be handy to allow the user to set real-time permissions to executable without needing root permission (i.e., without sudo). We can achieve this by adding the user to the realtime group. sudo addgroup realtime sudo usermod -a -G realtime $(whoami) Published on September 5, 2022 --- Last modified February 10, 2023"
  },"/2boot/2022-04-25-perf-tuning.html": {
    "title": "Tuning KVM for Best Performance",
    "keywords": "2boot, dual boot, kvm, linux, vm 2boot",
    "url": "/2boot/2022-04-25-perf-tuning.html",
    "body": "In the previous posts, we have set up a Kernel Virtual Machine (KVM) with libvirt and qemu. We have seen how type-I hypervisors allow almost bare-metal performance. With this post, I am going to talk about some tweaking and tuning we can do to further optimize KVM performance. Performance tuning is very well described in the Arch Linux wiki. Here we are going to see a few optimization steps to improve the performance of our VM. CPU pinning The first and easiest step is to enable CPU pinning. By default, KVM handles guests as normal threads representing virtual processors. These threads are managed by the Linux scheduler like any other thread, being assigned to any available CPU cores based on niceness and priority queues. As a result, the local CPU cache benefits (L1/L2/L3) are lost each time the host scheduler reschedules the virtual CPU thread on a different physical CPU. This can noticeably harm performance on the guest. CPU pinning aims to resolve this by limiting which physical CPUs the virtual CPUs are allowed to run on. The ideal setup is a 1:1 mapping such that the virtual CPU cores match physical CPU cores. lscpu -e shows the CPU topology: hyper-threading splits physical CPU cores (CORE) into virtual CPUs (CPU). CPU NODE SOCKET CORE L1d:L1i:L2:L3 ONLINE MAXMHZ MINMHZ 0 0 0 0 0:0:0:0 yes 4900.0000 400.0000 1 0 0 1 1:1:1:0 yes 4900.0000 400.0000 2 0 0 2 2:2:2:0 yes 4900.0000 400.0000 3 0 0 3 3:3:3:0 yes 4900.0000 400.0000 4 0 0 0 0:0:0:0 yes 4900.0000 400.0000 5 0 0 1 1:1:1:0 yes 4900.0000 400.0000 6 0 0 2 2:2:2:0 yes 4900.0000 400.0000 7 0 0 3 3:3:3:0 yes 4900.0000 400.0000 In my case (Intel i7-10510U), for example, the physical core 0 is split into virtual CPUs 0 and 4. As mentioned above, to optimize performance, we should passthrough virtual CPUs that correspond to physical CPU cores, sharing the lowest level CPU cache (L1 and L2). Core 0 should remain assigned to the host 1. To optimize performance, I have assigned vCPUs 2-7 to my Windows guest adding the following to the VM’s XML file. &lt;vcpu placement='static'&gt;6&lt;/vcpu&gt; &lt;cputune&gt; &lt;vcpupin vcpu='0' cpuset='1'/&gt; &lt;vcpupin vcpu='1' cpuset='5'/&gt; &lt;vcpupin vcpu='2' cpuset='2'/&gt; &lt;vcpupin vcpu='3' cpuset='6'/&gt; &lt;vcpupin vcpu='4' cpuset='3'/&gt; &lt;vcpupin vcpu='5' cpuset='7'/&gt; &lt;/cputune&gt; In the configuration, vcpu is the virtual CPU ID for the guest, while cpuset should correspond to CPU as reported by lscpu -e. I have left only CORE 0 to the Linux host, as I don’t plan to run heavy tasks on Linux while using the Windows guest. Frequency Scaling Modern CPUs do not run at a fixed clock frequency. They have a base clock, a minimum and a maximum (or turbo) frequency. Frequency modulation is a clever solution for the trade off between power saving and performance. These days, things are getting more and more complicated (partially because of nonsense marketing terminology). Let’s define the following: the minimum frequency is the lowest the CPU settles to at idle. This is 800 MHz on my Intel i7-10510U. Then, there is the nominal frequency (or base frequency), 1800 MHz in my case: let’s say this is the minimum frequency to which the CPU should be when not at idle. Depending on thermal headroom and load, the CPU frequency can increase in steps: the first step is the multi-core regular turbo frequency (2300 MHz), up to the absolute maximum turbo frequency (4900 MHz). The latter is often a single-core peak that may be reached only for brief bursts of power. The CPU will be mostly running at the regular turbo frequency under sustained, multi-core loads. This behavior is controlled by the CPU scaling governor. Modern Intel CPUs use the intel pstate governor 2. Frequency scaling can affect the performance of our virtual machines. Briefly, to assure optimal performance in our KVM guests, we should check whether CPU frequency scales correctly under an increased load from the guest. We can check the current CPU frequency in several ways. The command lscpu shows many information about our CPU, including the current frequency. If we want to look at per-core CPU frequency, this is stored in several files (as typically done in Linux). Specifically, this info can be accessed with: watch -n1 cat /sys/devices/system/cpu/cpu*/cpufreq/scaling_cur_freq The command watch -n1 will refresh the output of cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_cur_freq every second. Benchmarking Let us end this post with some benchmarks to quantify CPU performance tuning. I have chosen a very simple and quick benchmark with Prime95. I have chosen Prime95 for two reasons: it is multi-platform, thus it will run both on Linux and Windows, and it allows to select the number of CPU cores to stress test. I will test 3 cores with hyperthreading enabled both on Linux and on the Windows KVM. I have set up the throughput benchmark with a 2048K FFT for 15 seconds. I have run the test in several configurations: Default CPU topology: 2 sockets, 1 core, 1 thread (total of 2 vCPUs) Manual CPU topology w/o CPU pinning: 1 socket, 3 cores, 2 threads (total of 6 vCPUs) Manual CPU topology w/ CPU pinning: 1 socket, 3 cores, 2 threads (total of 6 vCPUs) Bare metal on Linux (total of 6 vCPUs) Of course, bare-metal performance (267.97 iter/sec) are higher than every KVM configuration. Nevertheless, we notice how manually tweaking CPU topology dramatically improves performance from 66.57 iter/sec up to &gt;200 iter/sec. CPU pinning further improves performance of ~13%. KVM performance with CPU pinning reaches almost 87% of the bare-metal performance. Below the full results for each configuration.   Worker 1 [ms] Worker 2 [ms] Worker 3 [ms] Throughput [iter/s] 1 32.60 27.86 / 66.57 2 15.47 14.41 14.22 204.31 3 13.40 12.73 12.80 231.31 4 14.60 14.17 7.76 267.97 Published on April 25, 2022 --- Last modified February 10, 2023"
  },"/2boot/2022-03-19-virtualize-partition.html": {
    "title": "Virtualize a Physical Windows Partition from Linux with KVM",
    "keywords": "2boot, dual boot, kvm, linux, vm 2boot",
    "url": "/2boot/2022-03-19-virtualize-partition.html",
    "body": "If you are familiar with a dual-boot configuration, you must have wondered at least once: is there a way I can avoid rebooting every time? Sometimes you just need to open an app that runs on Windows only for 5 minutes. Whatever the case may be, it would be very handy to be able to virtualize an existing partition. Please mind that this is not rocket science. It also isn’t something new. Yet, I remember I had to struggle a bit to get this working, and I could not find much online, so I hope this is helpful. Long-story short: yes, we can do this. We can virtualize the Windows partition from Linux. I.e., we can create a virtual machine and use the existing Windows partition as the drive for the VM. When I started off, I was familiar only with VirtualBox. Turns out it is not that hard to make it working: you just have to create a virtual disk that points both to the Windows partition and to the UEFI partition for boot. You can check out this post if you prefer to use VirtualBox (coming soon; in the meantime, check this out). Today, we are going to set this up using KVM and QEMU. KVM stands for Kernel Virtual Machine. The main advantage of KVM is performance. In short, “KVM converts Linux into a type-1 (bare-metal) hypervisor”. Hence, KVM can take advantage of the Linux kernel’s memory controller, process scheduler, I/O stack, network stack, and so on. This allows much higher performance with less over-head compared to a type-2 hypervisor (e.g., VirtualBox). Building the virtual disk We are going to use mdadm to create and manage the virtual RAID array. First, we need to check how our disk is structured. Please mind that here we are consindering a single-disk dual boot setup. We can use fdisk -l &lt;disk&gt; to check our disk. Here is how the output looks like in my case. $ fdisk -l /dev/nvme0n1 Disk /dev/nvme0n1: 477 GiB, 512110190592 bytes, 1000215216 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes (...) Device Start End Sectors Size Type /dev/nvme0n1p1 2048 1599487 1597440 780M EFI System /dev/nvme0n1p2 1599488 12085247 10485760 5G Microsoft reserved /dev/nvme0n1p3 12085248 821014527 808929280 385.7G Linux filesystem /dev/nvme0n1p4 821014528 998633471 177618944 84.7G Microsoft basic data We can identify the Windows partition as the one with the label nvme0n1p4. Now we need to create two files: one (100 MB) to hold the partition table and the EFI partition, and the second one (1 MB) to hold a backup of the partition table. Then, we can create a loop device to associate to each of these files with losetup: dd if=/dev/zero of=$HOME/efi1 bs=1M count=100 dd if=/dev/zero of=$HOME/efi2 bs=1M count=1 LOOP1=$(sudo losetup -f) sudo losetup ${LOOP1} $HOME/efi1 LOOP2=$(sudo losetup -f) sudo losetup ${LOOP2} $HOME/efi2 The creation of the files ef1 and efi2 is a one-time procedure, while losetup needs to be run every time we want to boot the VM. Finally, we can merge all these with the physical partition (/dev/nvme0n1p4) and create the RAID array: sudo mdadm --build --verbose /dev/md0 --chunk=512 \\ --level=linear --raid-devices=3 ${LOOP1} /dev/nvme0n1p4 ${LOOP2} Another one-time-only step is to create the partition table in the virtual RAID disk. We can do this with parted: sudo parted /dev/md0 (parted) unit s (parted) mktable gpt (parted) mkpart primary fat32 2048 204799 # depends on size of efi1 file (parted) mkpart primary ntfs 204800 -2049 # depends on size of efi1 and efi2 files (parted) set 1 boot on (parted) set 1 esp on (parted) set 2 msftdata on (parted) name 1 EFI (parted) name 2 Windows (parted) quit With these commands, we have created the EFI partition /dev/md0p1 and the Windows partition /dev/md0p2. We need to format the EFI partition with: sudo mkfs.msdos -F 32 -n EFI /dev/md0p1 The mdadm array is built and ready to be used. As a last step, we need to change its ownership to allow our user to perform R/W operations on it: sudo chown $USER:$USER /dev/md0. Creating a KVM with virt-manager We are going to use virt-manager (Virtual Machine Manager) to handle our QEMU/KVM virtual machine. The virt-manager application allows both GUI-based and XML-based configuration and uses libvirt for KVM machines. You can install with your packet manager, or from source. Please mind that depending on your distribution, you may get an older version. For example, sudo apt install virt-manager installs version 1.5.1 on Ubuntu 18.04 LTS, while the latest is 4.0.0 at the time of writing. The biggest difference I have noticed regards XML editing: newer versions have an embedded editor. For older versions, you need to virsh edit &lt;VM_NAME&gt; from the terminal. Launch virt-manager to create the virtual machine. We are going to need the Windows ISO even if we already have it installed in our partition. Make sure that “Connection” is QEMU/KVM and select “Import existing disk image”, as shown in the screenshot below. Specify the virtual RAID array we created before as the existing drive path, i.e., /dev/md0. You can go further with the configuration. At the end, make sure to select “Customize configuration before install” before clicking on “Finish”. Now we can further customize the VM. I recommend adding the Windows 10 ISO (Add Hardware &gt; Storage), as we will need it for the first boot. You may also edit the default configuration as you like. You can find my configuration at this Gist. I especially recommend the following configuration blocks. For the CPU, the best setting is host-passthrough: &lt;cpu mode='host-passthrough' check='none'&gt; &lt;topology sockets='1' cores='4' threads='2'/&gt; &lt;/cpu&gt; Next, we may change the clock configuration: &lt;clock offset='localtime'&gt; &lt;timer name='hpet' present='yes'/&gt; &lt;timer name='hypervclock' present='yes'/&gt; &lt;/clock&gt; You can also check this guide if you need step-by-step instructions. First boot In the MDADM drive array we created before, we have wrapped the physical Windows partition with a virtual EFI/UEFI partition. Hence, we need to have a virtual EFI firmware on our computer. We are going to use OVMF for this purpose. First, check the os tag of the XML configuration and make sure the VM is using OVMF. &lt;os&gt; &lt;type arch='x86_64' machine='pc-i440fx-bionic'&gt;hvm&lt;/type&gt; &lt;loader readonly='yes' type='pflash'&gt;/usr/share/OVMF/OVMF_CODE.fd&lt;/loader&gt; &lt;nvram template='/usr/share/OVMF/OVMF_VARS.fd'&gt;/var/lib/libvirt/qemu/nvram/Win10_VARS.fd&lt;/nvram&gt; &lt;bootmenu enable='yes'/&gt; &lt;/os&gt; The loader (i.e., the virtual EFI firmware) is the file OVMF_CODE.fd. The second file is for virtual RAM. You may check your paths with find / -name \"OVMF_*\" 2&gt;/dev/null. If you can’t find it, you can install it with your packet manager (e.g., sudo apt install ovmf). In case of issues, checkout this guide. Make sure you have your SATA CDROM1 at the top of the Boot Options and click on “Begin Installation”. The machine should boot to the Windows installation disk. We need to assign a letter to the EFI partition to make the VM boot. If you followed the previous post, you should already have Windows installed. We just need to assign a letter to the EFI partition with ðiskpart. To do this, press Shift+F10 to open up a Windows terminal. diskpart DISKPART&gt; list disk DISKPART&gt; select disk 0 # Select the disk DISKPART&gt; list volume # Find EFI volume (partition) number DISKPART&gt; select volume 2 # Select EFI volume DISKPART&gt; assign letter=B # Assign B: to EFI volume DISKPART&gt; exit The last touch is to copy the BCD boot entry for the Windows partition to the volume we just created. From the same terminal, we can run: bcdboot C:\\Windows /s B: /f ALL We can now shutdown the machine, remove the Windows ISO and boot again. Finally – after a very long post – we are ready to boot our physical Windows partition! In case of trouble, you may debug your setup by launching directly QEMU from the terminal, by-passing virt-manager: qemu-system-x86_64 \\ -bios /usr/share/OVMF/OVMF_CODE.fd \\ # Use OVMF -drive file=/dev/md0,media=disk,format=raw \\ # Boot from /dev/md0 -cpu host -enable-kvm \\ # Copy host configuration for CPU -m 2G # RAM (2 GB) Wrap-up To conclude, it is worth clearing up the ideas. We have created our KVM and booted the physical Windows partition under Linux. As mentioned, we need to create the mdadm array every time we reboot our physical machine. For convenience, I have set up this bash script. In case you need to mount the Windows partition (of course, when the VM is not running!!), you may stop the array with sudo mdadm --stop /dev/md0 before. In a future post, I will talk about an AppIndicator that I have been developing to help managing the KVM from Ubuntu/Linux.     References Boot Your Windows Partition from Linux using KVM | jianmin|dev Boot physical Windows inside Qemu guest machine | Moez Bouhlel [lejenome] Published on March 19, 2022 --- Last modified February 10, 2023"
  },"/2boot/2022-03-05-winux.html": {
    "title": "Windows+Linux Dual Boot",
    "keywords": "2boot, dual boot, kvm, linux, vm 2boot",
    "url": "/2boot/2022-03-05-winux.html",
    "body": "Setting up a regular dual boot configuration is quite easy. It is trivial if you are installing Linux on a partition of a Windows PC. It may be a little bit trickier the other way around. This is because Windows will overwrite the boot sector with its (stupid) MBR, throwing GRUB out of the window. We can fix this either manually, or we could use boot-repair. We need to add its PPA before installing it with apt: sudo add-apt-repository ppa:yannubuntu/boot-repair sudo apt update sudo apt install boot-repair To repair the boot sector, run boot-repair from the terminal and click on Recommended repair. Once finished, don’t forget to run sudo update-grub.   That’s it. You should now have a regular dual-boot configuration with Windows and your favorite Linux distribution side by side. In the next posts, we are going to see how we can turn into something much more interesting. Published on March 5, 2022 --- Last modified February 10, 2023"
  },"/2boot/2022-03-05-dual-boot.html": {
    "title": "Dual Boot or Virtual Machine? Easy, both!",
    "keywords": "2boot, dual boot, kvm, linux, vm 2boot",
    "url": "/2boot/2022-03-05-dual-boot.html",
    "body": "At the starting of my PhD in Bioengineering, back in 2019, I was running a 6-years-old MacBook Air. It served me well during my bachelors and my masters, but an upgrade was overdue. I needed a laptop for work. I did not want to use Windows. I really could not go back. I chose the new Dell XPS 13, the developer edition that runs Ubuntu (18.04) LTS out of the box. A great option to save some bucks, if you don’t need Windows or you can get a license at work. Let’s be honest here. Either you like it or not, you still need Windows. I have some Windows-only software that I need to run (e.g., PSoC Creator). There is also software that does not run (as well as on Windows) on Linux. Everything developed by Microsoft, for example. Office 365 is quite good these days, and well integrated with OneDrive. The web version just sucks. So here I am, downloading the Windows 10 ISO on my brand-new Ubuntu-powered laptop. Is it over already? Virtualization -vs- Dual Boot We all know we have two options. If you have enough under the hood, you can just virtualize Windows and run it inside Linux. This option is fine when you need to use Windows-based programs once in a while, a few apps at a time (by the way, I already recommend checking out WinApps. The dual option (pun intended) is setting up a dual boot. If you are not limited by disk space here, you can just reboot and fire up Windows, enjoying bare-metal performance. This makes sense if one day you need to work on Windows, and you have there everything you need. This may result in frustratingly having to reboot several times per day. There are many factors to include trying to figure out what’s best: energy efficiency, memory efficiency, computational power, flexibility… So then I figured, I could have both. Dual boot Windows and Linux, and have a Windows VM inside Linux. Do I like this solution? No. I started wondering: can I virtualize my Windows partition? So I started digging online… and this came out. As of the time of writing, I am successfully running what I have been calling 2Boot. It’s like Dual Boot v2.0. I have Ubuntu 18.04 LTS and Windows 10, side-by-side in a classic dual boot configuration. What’s special (if anything) is that I can virtualize my Windows partition and fire up the very same Win10 machine as a virtual machine inside Linux, with all the data and programs installed. This is great for flexibility and memory efficiency: not a single precious GB of the NVME SSD is wasted on a Virtual Disk Image. This post is already quite long. So I will split it into several posts that will go under the category 2boot. Stay tuned! Published on March 5, 2022 --- Last modified February 10, 2023"
  },"/meta/2022-02-21-hello-world.html": {
    "title": "Hello World",
    "keywords": "meta",
    "url": "/meta/2022-02-21-hello-world.html",
    "body": "Hello, world! Here is it. You can find out more about me here. Nothing much to say here. Published on February 21, 2022"
  }}
